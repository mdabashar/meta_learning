{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "\n",
    "[1] https://github.com/learnables/learn2learn/blob/master/examples/text/news_topic_classification.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install fairseq --user\n",
    "# !pip install learn2learn --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification using MAML implemented through learn2learn library\n",
    "\n",
    "learn2learn is a software library for meta-learning research. It is built on top of PyTorch to accelerate two aspects of the meta-learning research cycle:\n",
    "\n",
    "1. fast prototyping, essential in letting researchers quickly try new ideas, and\n",
    "2. correct reproducibility, ensuring that these ideas are evaluated fairly.\n",
    "\n",
    "MAML is a Model-Agnostic Meta-Learning Algorithm proposed in https://arxiv.org/pdf/1703.03400.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "import learn2learn as l2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\basharm\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"Head for sentence-level classification tasks.\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes, input_dim=768, inner_dim=200, pooler_dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(input_dim, inner_dim)\n",
    "        self.activation_fn = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=pooler_dropout)\n",
    "        self.out_proj = nn.Linear(inner_dim, num_classes)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.activation_fn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.log_softmax(self.out_proj(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Accuracy Calculation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, targets):\n",
    "    predictions = predictions.argmax(dim=1)\n",
    "    acc = (predictions == targets).sum().float()\n",
    "    acc /= len(targets)\n",
    "    return acc.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Utilisty Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_tokens(values, pad_idx, eos_idx=None, left_pad=False, move_eos_to_beginning=False):\n",
    "    \"\"\"Convert a list of 1d tensors into a padded 2d tensor.\"\"\"\n",
    "    size = max(v.size(0) for v in values)\n",
    "    res = values[0].new(len(values), size).fill_(pad_idx)\n",
    "\n",
    "    def copy_tensor(src, dst):\n",
    "        assert dst.numel() == src.numel()\n",
    "        if move_eos_to_beginning:\n",
    "            assert src[-1] == eos_idx\n",
    "            dst[0] = eos_idx\n",
    "            dst[1:] = src[:-1]\n",
    "        else:\n",
    "            dst.copy_(src)\n",
    "\n",
    "    for i, v in enumerate(values):\n",
    "        copy_tensor(v, res[i][size - len(v):] if left_pad else res[i][:len(v)])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _BatchedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, batched):\n",
    "        self.sents = [s for s in batched[0]]\n",
    "        self.ys = [y for y in batched[1]]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ys)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.sents[idx], self.ys[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(task, roberta, device, learner, loss_func, batch=15):\n",
    "    loss = 0.0\n",
    "    acc = 0.0\n",
    "    for i, (x, y) in enumerate(torch.utils.data.DataLoader(\n",
    "            _BatchedDataset(task), batch_size=batch, shuffle=True, num_workers=0)):\n",
    "        # RoBERTa ENCODING\n",
    "        x = collate_tokens([roberta.encode(sent) for sent in x], pad_idx=1)\n",
    "        with torch.no_grad():\n",
    "            x = roberta.extract_features(x)\n",
    "        x = x[:, 0, :]\n",
    "\n",
    "        # Moving to device\n",
    "        x, y = x.to(device), y.view(-1).to(device)\n",
    "\n",
    "        output = learner(x)\n",
    "        curr_loss = loss_func(output, y)\n",
    "        acc += accuracy(output, y)\n",
    "        loss += curr_loss / len(task)\n",
    "    loss /= len(task)\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the function that will be doing the training of meta learniing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(lr=0.005, maml_lr=0.01, iterations=1000, ways=5, shots=1, tps=32, fas=5, device=torch.device(\"cpu\"),\n",
    "         download_location=\"/tmp/text\"):\n",
    "    dataset = l2l.text.datasets.NewsClassification(root=download_location, download=True)\n",
    "    dataset = l2l.data.MetaDataset(dataset)\n",
    "\n",
    "    classes = list(range(len(dataset.labels))) # 41 classes\n",
    "    random.shuffle(classes)\n",
    "\n",
    "    train_dataset, validation_dataset, test_dataset = dataset, dataset, dataset\n",
    "\n",
    "    train_gen = l2l.data.TaskDataset(\n",
    "            train_dataset, num_tasks=20000, \n",
    "            task_transforms=[\n",
    "                l2l.data.transforms.FusedNWaysKShots(\n",
    "                    train_dataset, n=ways, k=shots, filter_labels=classes[:20]),\n",
    "                l2l.data.transforms.LoadData(train_dataset),\n",
    "                l2l.data.transforms.RemapLabels(train_dataset)],)\n",
    "\n",
    "    validation_gen = l2l.data.TaskDataset(\n",
    "            validation_dataset, num_tasks=20000, \n",
    "            task_transforms=[\n",
    "                l2l.data.transforms.FusedNWaysKShots(\n",
    "                    validation_dataset, n=ways, k=shots, filter_labels=classes[20:30]),\n",
    "                l2l.data.transforms.LoadData(validation_dataset),\n",
    "                l2l.data.transforms.RemapLabels(validation_dataset)],)\n",
    "\n",
    "    test_gen = l2l.data.TaskDataset(\n",
    "            test_dataset, num_tasks=20000, \n",
    "            task_transforms=[\n",
    "                l2l.data.transforms.FusedNWaysKShots(\n",
    "                    test_dataset, n=ways, k=shots, filter_labels=classes[30:]),\n",
    "                l2l.data.transforms.LoadData(test_dataset),\n",
    "                l2l.data.transforms.RemapLabels(test_dataset)],)\n",
    "\n",
    "    torch.hub.set_dir(download_location)\n",
    "    roberta = torch.hub.load('pytorch/fairseq', 'roberta.base')\n",
    "    roberta.eval()\n",
    "    roberta.to(device)\n",
    "    model = Net(num_classes=ways)\n",
    "    model.to(device)\n",
    "    meta_model = l2l.algorithms.MAML(model, lr=maml_lr)\n",
    "    opt = optim.Adam(meta_model.parameters(), lr=lr)\n",
    "    loss_func = nn.NLLLoss(reduction=\"sum\")\n",
    "\n",
    "    tqdm_bar = tqdm(range(iterations))\n",
    "\n",
    "    accs = []\n",
    "    for _ in tqdm_bar:\n",
    "        iteration_error = 0.0\n",
    "        iteration_acc = 0.0\n",
    "        for _ in range(tps):\n",
    "            learner = meta_model.clone()\n",
    "            train_task, valid_task = train_gen.sample(), validation_gen.sample()\n",
    "\n",
    "            # Fast Adaptation\n",
    "            for _ in range(fas):\n",
    "                train_error, _ = compute_loss(train_task, roberta, device, learner, loss_func, batch=shots * ways)\n",
    "                learner.adapt(train_error)\n",
    "\n",
    "            # Compute validation loss\n",
    "            valid_error, valid_acc = compute_loss(valid_task, roberta, device, learner, loss_func,\n",
    "                                                  batch=shots * ways)\n",
    "            iteration_error += valid_error\n",
    "            iteration_acc += valid_acc\n",
    "\n",
    "        iteration_error /= tps\n",
    "        iteration_acc /= tps\n",
    "        tqdm_bar.set_description(\"Loss : {:.3f} Acc : {:.3f}\".format(iteration_error.item(), iteration_acc))\n",
    "        accs.append(iteration_acc)\n",
    "        # Take the meta-learning step\n",
    "        opt.zero_grad()\n",
    "        iteration_error.backward()\n",
    "        opt.step()\n",
    "    print (f'first and best validation accuracy: {accs[0]:.4f}, {max(accs):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.ways  = 5 # number of ways (default: 5)\n",
    "        self.shots = 1 # number of shots (default: 1)\n",
    "        self.tasks_per_step = 32 # tasks per step (default: 32)\n",
    "        self.fast_adaption_steps = 5 # steps per fast adaption (default: 5)\n",
    "        self.iterations = 2 # number of iterations (default: 1000)\n",
    "        self.lr = 0.005 # learning rate (default: 0.005)\n",
    "        self.maml_lr = 0.01 # learning rate for MAML (default: 0.01)\n",
    "        self.no_cuda = False # disables CUDA training\n",
    "        self.seed = 1 # random seed (default: 1)\n",
    "        self.download_location = '/tmp/text' # download location for train data and roberta(default : /tmp/text\n",
    "\n",
    "args = Arguments()\n",
    "args.lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /tmp/text\\pytorch_fairseq_master\n",
      "C:\\Users\\basharm\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\proto\\tensor_shape_pb2.py:18: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "C:\\Users\\basharm\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\proto\\tensor_shape_pb2.py:36: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.FieldDescriptor(\n",
      "C:\\Users\\basharm\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\proto\\tensor_shape_pb2.py:29: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _TENSORSHAPEPROTO_DIM = _descriptor.Descriptor(\n",
      "C:\\Users\\basharm\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\proto\\types_pb2.py:19: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "C:\\Users\\basharm\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\proto\\types_pb2.py:33: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.EnumValueDescriptor(\n",
      "C:\\Users\\basharm\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\proto\\types_pb2.py:27: DeprecationWarning: Call to deprecated create function EnumDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _DATATYPE = _descriptor.EnumDescriptor(\n",
      "C:\\Users\\basharm\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\proto\\resource_handle_pb2.py:20: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "C:\\Users\\basharm\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\proto\\resource_handle_pb2.py:39: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.FieldDescriptor(\n",
      "C:\\Users\\basharm\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\proto\\resource_handle_pb2.py:32: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _RESOURCEHANDLEPROTO_DTYPEANDSHAPE = _descriptor.Descriptor(\n",
      "C:\\Users\\basharm\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\proto\\tensor_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "C:\\Users\\basharm\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\proto\\tensor_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.FieldDescriptor(\n",
      "C:\\Users\\basharm\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\proto\\tensor_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _TENSORPROTO = _descriptor.Descriptor(\n",
      "C:\\Users\\basharm\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\proto\\summary_pb2.py:20: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "C:\\Users\\basharm\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\proto\\summary_pb2.py:35: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.EnumValueDescriptor(\n",
      "C:\\Users\\basharm\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\proto\\summary_pb2.py:29: DeprecationWarning: Call to deprecated create function EnumDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _DATACLASS = _descriptor.EnumDescriptor(\n",
      "C:\\Users\\basharm\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\proto\\summary_pb2.py:74: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.FieldDescriptor(\n",
      "C:\\Users\\basharm\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\proto\\summary_pb2.py:67: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _SUMMARYDESCRIPTION = _descriptor.Descriptor(\n",
      "2022-09-05 23:16:23 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2022-09-05 23:16:24 | INFO | fairseq.file_utils | loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.base.tar.gz from cache at C:\\Users\\basharm\\.cache\\torch\\pytorch_fairseq\\37d2bc14cf6332d61ed5abeb579948e6054e46cc724c7d23426382d11a31b2d6.ae5852b4abc6bf762e0b6b30f19e741aa05562471e9eb8f4a6ae261f04f9b350\n",
      "2022-09-05 23:16:28 | INFO | fairseq.tasks.masked_lm | dictionary: 50264 types\n",
      "2022-09-05 23:16:30 | INFO | fairseq.models.roberta.model | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 512, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19812, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 999999, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 999999, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0006], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 512}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='roberta_base', activation_dropout=0.0, activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, arch='roberta_base', attention_dropout=0.1, best_checkpoint_metric='loss', bpe='gpt2', bucket_cap_mb=200, clip_norm=0.0, cpu=False, criterion='masked_lm', curriculum=0, data='C:\\\\Users\\\\basharm\\\\.cache\\\\torch\\\\pytorch_fairseq\\\\37d2bc14cf6332d61ed5abeb579948e6054e46cc724c7d23426382d11a31b2d6.ae5852b4abc6bf762e0b6b30f19e741aa05562471e9eb8f4a6ae261f04f9b350', dataset_impl='mmap', ddp_backend='c10d', device_id=0, disable_validation=False, distributed_backend='nccl', distributed_no_spawn=False, distributed_port=19812, distributed_rank=0, distributed_world_size=512, dropout=0.1, encoder_attention_heads=12, encoder_embed_dim=768, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=12, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, ffn_blocks_to_remove=-1, ffn_reg_scale_factor=0.0, find_unused_parameters=True, fix_batches_to_gpus=False, force_anneal=None, fp16=True, fp16_init_scale=4, fp16_scale_tolerance=0.0, fp16_scale_window=128, global_sync_iter=10, keep_interval_updates=-1, keep_last_epochs=-1, layernorm_embedding=True, leave_unmasked_prob=0.1, load_checkpoint_heads=True, log_format='json', log_interval=25, lr=[0.0006], lr_scheduler='polynomial_decay', mask_prob=0.15, max_epoch=0, max_positions=512, max_sentences=16, max_sentences_valid=16, max_source_positions=512, max_target_positions=512, max_tokens=999999, max_update=500000, maximize_best_checkpoint_metric=False, memory_efficient_fp16=True, mha_heads_to_keep=-1, mha_reg_scale_factor=0.0, min_loss_scale=0.0001, min_params_to_wrap=100000000, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_token_positional_embeddings=False, num_workers=2, only_validate=False, optimizer='adam', optimizer_overrides='{}', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, random_token_prob=0.1, required_batch_size_multiple=1, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sample_break_mode='complete', save_interval=1, save_interval_updates=2000, seed=1, sentence_avg=False, skip_invalid_size_inputs_valid_test=True, spectral_norm_classification_head=False, stop_min_lr=-1, task='masked_lm', tbmf_wrapper=False, threshold_loss_scale=1.0, tokenizer=None, tokens_per_sample=512, total_num_update=500000, train_subset='train', untie_weights_roberta=False, update_freq=[1], use_bmuf=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_updates=24000, weight_decay=0.01), 'task': {'_name': 'masked_lm', 'data': 'C:\\\\Users\\\\basharm\\\\.cache\\\\torch\\\\pytorch_fairseq\\\\37d2bc14cf6332d61ed5abeb579948e6054e46cc724c7d23426382d11a31b2d6.ae5852b4abc6bf762e0b6b30f19e741aa05562471e9eb8f4a6ae261f04f9b350', 'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 'seed': 1, 'include_target_tokens': False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': True, 'lr': [0.0006]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0, 'lr': [0.0006]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss : 2.016 Acc : 0.206: 100%|████████████████████████████████████████████| 2/2 [00:50<00:00, 25.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first and best validation accuracy: 0.2000, 0.2063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "main(lr=args.lr, maml_lr=args.maml_lr, iterations=args.iterations, ways=args.ways, shots=args.shots,\n",
    "     tps=args.tasks_per_step, fas=args.fast_adaption_steps, device=device,\n",
    "     download_location=args.download_location)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
