{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "\n",
    "[1] https://github.com/learnables/learn2learn/blob/master/examples/text/news_topic_classification.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install fairseq --user\n",
    "# !pip install learn2learn --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta Learning MAML for Abuse Detection in Tweets [implemented through learn2learn library]\n",
    "\n",
    "learn2learn is a software library for meta-learning research. It is built on top of PyTorch to accelerate two aspects of the meta-learning research cycle:\n",
    "\n",
    "1. fast prototyping, essential in letting researchers quickly try new ideas, and\n",
    "2. correct reproducibility, ensuring that these ideas are evaluated fairly.\n",
    "\n",
    "MAML is a Model-Agnostic Meta-Learning Algorithm proposed in https://arxiv.org/pdf/1703.03400.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "import learn2learn as l2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\basharm\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "BASE = 'C:\\\\Users\\\\basharm\\\\OneDrive - Queensland University of Technology\\\\JupyterPythonQUT\\\\StandardDataSets\\\\QMI\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = 'train.csv'\n",
    "test = 'test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>@ nbcnews a blowhard , ignorant reality tv sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@ biushydolan listen ðŸ˜” im not a whore for whit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>kyle scrolls bye my photo on insta and goes â€œ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>due to few evil-doers , 65 crore male populati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>@ dloesch @ maryemcglynn @ alyssa_milano @ hea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      1  @ nbcnews a blowhard , ignorant reality tv sho...\n",
       "1      0  @ biushydolan listen ðŸ˜” im not a whore for whit...\n",
       "2      0  kyle scrolls bye my photo on insta and goes â€œ ...\n",
       "3      0  due to few evil-doers , 65 crore male populati...\n",
       "4      1  @ dloesch @ maryemcglynn @ alyssa_milano @ hea..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(BASE+train)\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\basharm\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       text\n",
       "label      \n",
       "0      2572\n",
       "1      1439"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\basharm\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# We apply only this preprocessing because our data is already preprocessed\n",
    "def cleanNonAscii(text):\n",
    "    return ''.join(i for i in text if ord(i) < 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text'] = df_train['text'].apply(cleanNonAscii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = df_train['text'].values, df_train['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>@ winegirl73 2 bit attention seeking whore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Â“ i was dodging bullets in slow motion you slut Â”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@ mayoroflondon sadiq khan-proud mayor of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>@ lewdpon3 i agree , you should definitely lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>@ peoples_pundit @ fluffydogattack @ sengillib...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      1         @ winegirl73 2 bit attention seeking whore\n",
       "1      0  Â“ i was dodging bullets in slow motion you slut Â”\n",
       "2      0  @ mayoroflondon sadiq khan-proud mayor of the ...\n",
       "3      0  @ lewdpon3 i agree , you should definitely lea...\n",
       "4      1  @ peoples_pundit @ fluffydogattack @ sengillib..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(BASE+test)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\basharm\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "df_test['text'] = df_test['text'].apply(cleanNonAscii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = df_test['text'].values, df_test['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('@ nbcnews a blowhard , ignorant reality tv show host and his porn whore russian wife . perfect .',\n",
       " '@ winegirl73 2 bit attention seeking whore')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0], X_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize text and pad sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\basharm\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:18: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "C:\\Users\\basharm\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:36: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.FieldDescriptor(\n",
      "C:\\Users\\basharm\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:29: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _TENSORSHAPEPROTO_DIM = _descriptor.Descriptor(\n",
      "C:\\Users\\basharm\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:19: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "C:\\Users\\basharm\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:33: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.EnumValueDescriptor(\n",
      "C:\\Users\\basharm\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:27: DeprecationWarning: Call to deprecated create function EnumDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _DATATYPE = _descriptor.EnumDescriptor(\n",
      "C:\\Users\\basharm\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:20: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "C:\\Users\\basharm\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:39: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.FieldDescriptor(\n",
      "C:\\Users\\basharm\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:32: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _RESOURCEHANDLEPROTO_DTYPEANDSHAPE = _descriptor.Descriptor(\n",
      "C:\\Users\\basharm\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "C:\\Users\\basharm\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.FieldDescriptor(\n",
      "C:\\Users\\basharm\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _TENSORPROTO = _descriptor.Descriptor(\n",
      "C:\\Users\\basharm\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "C:\\Users\\basharm\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.FieldDescriptor(\n",
      "C:\\Users\\basharm\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _ATTRVALUE_LISTVALUE = _descriptor.Descriptor(\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "num_words = 100000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7533, 10, 317]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences([\"physics is nice\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7533, 10, 317]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences([\"physics is nice\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['physics is nice']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[7533, 10, 317]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = tokenizer.texts_to_sequences(X_train)\n",
    "maxlen = max(map(lambda x: len(x),xtrain))\n",
    "xtrain = pad_sequences(xtrain, maxlen=maxlen)\n",
    "\n",
    "xtest = tokenizer.texts_to_sequences(X_test)\n",
    "xtest = pad_sequences(xtest, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4011, 54), (1003, 54))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain.shape, xtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2, 2970,  259, 1064,  480,  260, 2971,    8,   89,  332,    1,\n",
       "        446,  229,  527])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain[0][40:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (dense): Linear(in_features=54, out_features=200, bias=True)\n",
       "  (activation_fn): ReLU()\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (out_proj): Linear(in_features=200, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"Head for sentence-level classification tasks.\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes, input_dim=54, inner_dim=200, pooler_dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(input_dim, inner_dim)\n",
    "        self.activation_fn = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=pooler_dropout)\n",
    "        self.out_proj = nn.Linear(inner_dim, num_classes)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        #x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.activation_fn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.log_softmax(self.out_proj(x), dim=1)\n",
    "        return x\n",
    "    \n",
    "model = Net(2)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Accuracy Calculation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, targets):\n",
    "    predictions = predictions.argmax(dim=1)\n",
    "    acc = (predictions == targets).sum().float()\n",
    "    acc /= len(targets)\n",
    "    return acc.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Utilisty Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _BatchedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, xbatch, ybatch):\n",
    "        self.xbatch = xbatch\n",
    "        self.ybatch = ybatch\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ybatch)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.xbatch[idx], self.ybatch[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xtask = xtrain\n",
    "# ytask = y_train\n",
    "# torch.utils.data.DataLoader(_BatchedDataset(xtask, ytask), batch_size=5, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(task, device, learner, loss_func, batch=10):\n",
    "    loss = 0.0\n",
    "    acc = 0.0\n",
    "    xtask, ytask = task\n",
    "    for i, (x, y) in enumerate(torch.utils.data.DataLoader(\n",
    "            _BatchedDataset(xtask, ytask), batch_size=batch, shuffle=True, num_workers=0)):\n",
    "        \n",
    "        # Moving to device\n",
    "        x = x.to(torch.float32)\n",
    "        x, y = x.to(device), y.view(-1).to(device)\n",
    "\n",
    "        output = learner(x) # learner is the classification model\n",
    "        curr_loss = loss_func(output, y)\n",
    "        acc += accuracy(output, y)\n",
    "        loss += curr_loss / len(ytask)\n",
    "    loss /= len(ytask)\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = l2l.data.MetaDataset(_BatchedDataset(xtrain, y_train))\n",
    "classes = list(range(len(dataset.labels))) # 41 classes\n",
    "random.shuffle(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, validation_dataset, test_dataset = dataset, dataset, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ways = 2\n",
    "shots = 100\n",
    "\n",
    "transforms = [\n",
    "    l2l.data.transforms.NWays(train_dataset, n=ways),\n",
    "    l2l.data.transforms.KShots(train_dataset, k=shots),\n",
    "    l2l.data.transforms.LoadData(train_dataset),\n",
    "]\n",
    "train_gen = l2l.data.TaskDataset(train_dataset, transforms, num_tasks=len(xtrain)/shots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = [\n",
    "    l2l.data.transforms.NWays(validation_dataset, n=ways),\n",
    "    l2l.data.transforms.KShots(validation_dataset, k=shots),\n",
    "    l2l.data.transforms.LoadData(validation_dataset),\n",
    "]\n",
    "valid_gen = l2l.data.TaskDataset(validation_dataset, transforms, num_tasks=len(xtrain)/shots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = [\n",
    "    l2l.data.transforms.NWays(test_dataset, n=ways),\n",
    "    l2l.data.transforms.KShots(test_dataset, k=shots),\n",
    "    l2l.data.transforms.LoadData(test_dataset),\n",
    "]\n",
    "test_gen = l2l.data.TaskDataset(test_dataset, transforms, num_tasks=len(xtrain)/shots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "maml_lr = 0.01\n",
    "meta_model = l2l.algorithms.MAML(model, lr=maml_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.005\n",
    "opt = optim.Adam(meta_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.NLLLoss(reduction=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss : 7.561 Acc : 0.576: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [04:24<00:00,  3.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first and best validation accuracy: 0.5110, 0.6080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epocs = 1000\n",
    "tqdm_bar = tqdm(range(epocs))\n",
    "\n",
    "accs = []\n",
    "tps = 10\n",
    "fas = 5\n",
    "for _ in tqdm_bar:\n",
    "    iteration_error = 0.0\n",
    "    iteration_acc = 0.0\n",
    "    for _ in range(tps): # tps = tasks per step\n",
    "        learner = meta_model.clone()\n",
    "        train_task, valid_task = train_gen.sample(), valid_gen.sample()\n",
    "\n",
    "        # Fast Adaptation\n",
    "        for _ in range(fas): # # steps per fast adaption (default: 5)\n",
    "            train_error, _ = compute_loss(train_task, device, learner, loss_func, batch=shots * ways)\n",
    "            learner.adapt(train_error)\n",
    "\n",
    "        # Compute validation loss\n",
    "        valid_error, valid_acc = compute_loss(valid_task, device, learner, loss_func,\n",
    "                                              batch=shots * ways)\n",
    "        iteration_error += valid_error\n",
    "        iteration_acc += valid_acc\n",
    "\n",
    "    iteration_error /= tps\n",
    "    iteration_acc /= tps\n",
    "    tqdm_bar.set_description(\"Loss : {:.3f} Acc : {:.3f}\".format(iteration_error.item(), iteration_acc))\n",
    "    accs.append(iteration_acc)\n",
    "    # Take the meta-learning step\n",
    "    opt.zero_grad()\n",
    "    iteration_error.backward()\n",
    "    opt.step()\n",
    "print (f'first and best validation accuracy: {accs[0]:.4f}, {max(accs):.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
