{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta Learning Algorithm MAML : A Simple Example from the Scratch\n",
    "\n",
    "MAML obtains a better and robust model parameter set $\\theta$ that is generalizable across tasks. To understand MAML, we will be coding it from scratch using only numpy. For simplicity, we consider a simple binary classification task. \n",
    "\n",
    "We randomly generate our input data and we train them with a simple single layer neural network and try to find the optimal parameter $\\theta$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all the necessary libraries,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate random Data Points\n",
    "\n",
    "We define a function called sample_points for generating our input $(x,y)$ pairs. It takes the parameter $k$ as an input which implies number of $(x,y)$ pairs we want to sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_points(k):\n",
    "    x = np.random.rand(k,50)\n",
    "    y = np.random.choice([0, 1], size=k, p=[.5, .5]).reshape([-1,1])\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.60300192 0.45776516 0.02132433 0.51366272 0.44169255 0.08744552\n",
      " 0.45726669 0.71399018 0.48181505 0.71072881 0.42067071 0.66395623\n",
      " 0.33329473 0.97220175 0.94200933 0.9361742  0.01495021 0.01352856\n",
      " 0.6758419  0.40559623 0.6160479  0.10493967 0.23402734 0.51072059\n",
      " 0.93470721 0.87200238 0.86031737 0.50365179 0.34689992 0.03540216\n",
      " 0.33998329 0.98409543 0.87289698 0.53423716 0.42945466 0.57573521\n",
      " 0.69726405 0.52342714 0.37637587 0.45299253 0.32280048 0.41412377\n",
      " 0.97935162 0.18538661 0.26884895 0.62419163 0.75461915 0.89480538\n",
      " 0.41472785 0.63732779]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# The above function returns output as follows\n",
    "x, y = sample_points(10)\n",
    "print(x[0])\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAML with Single Layer Neural Network\n",
    "\n",
    "For simplicity, we use a neural network with only single layer for predicting the output. i.e,\n",
    "\n",
    "a = np.matmul(X, theta)\n",
    "\n",
    "YHat = sigmoid(a)\n",
    "\n",
    "We use MAML for finding this optimal parameter value theta that is generalizable across tasks. So that for a new task, we can learn from a few data points in a lesser time by taking very less gradient steps.\n",
    "\n",
    "We define a class called MAML where we implement the MAML algorithm. In the __init__ method we will initialize all the necessary variables. Then we define our sigmoid activation function. Followed by we define our train function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple_MAML(object):\n",
    "    def __init__(self):\n",
    "        \n",
    "        #initialize number of tasks i.e number of tasks we need in each batch of tasks\n",
    "        self.num_tasks = 2\n",
    "        \n",
    "        #number of samples i.e number of shots  -number of data points (k) we need to have in each task\n",
    "        self.num_samples = 10\n",
    "\n",
    "        #number of epochs i.e training iterations\n",
    "        self.epochs = 100\n",
    "        \n",
    "        #hyperparameter for the inner loop (inner gradient update)\n",
    "        self.alpha = 0.0001\n",
    "        \n",
    "        #hyperparameter for the outer loop (outer gradient update) i.e meta optimization\n",
    "        self.beta = 0.0001\n",
    "       \n",
    "        #randomly initialize our model parameter theta\n",
    "        self.theta = np.random.normal(size=50).reshape(50, 1)\n",
    "      \n",
    "    #define our sigmoid activation function  \n",
    "    def sigmoid(self,a):\n",
    "        return 1.0 / (1 + np.exp(-a))\n",
    "    \n",
    "    \n",
    "    #now let us get to the interesting part i.e training :P\n",
    "    def train(self):\n",
    "        \n",
    "        #for the number of epochs,\n",
    "        for e in range(self.epochs):        \n",
    "            \n",
    "            self.theta_ = []\n",
    "            \n",
    "            #for storing gradient updates\n",
    "            self.g = []\n",
    "            \n",
    "            #for task i in batch of tasks\n",
    "            for i in range(self.num_tasks):\n",
    "               \n",
    "                #sample k data points and prepare our train set\n",
    "                XTrain, YTrain = sample_points(self.num_samples)\n",
    "                \n",
    "                a = np.matmul(XTrain, self.theta)\n",
    "\n",
    "                YHat = self.sigmoid(a)\n",
    "\n",
    "                #since we are performing classification, we use cross entropy loss as our loss function\n",
    "                loss = ((np.matmul(-YTrain.T, np.log(YHat)) - np.matmul((1 -YTrain.T), np.log(1 - YHat)))/self.num_samples)[0][0]\n",
    "                \n",
    "                #minimize the loss by calculating gradients\n",
    "                gradient = np.matmul(XTrain.T, (YHat - YTrain)) / self.num_samples\n",
    "\n",
    "                #update the gradients and find the optimal parameter theta' for each of tasks\n",
    "                self.theta_.append(self.theta - self.alpha*gradient)\n",
    "                \n",
    "                #compute the gradient update\n",
    "                self.g.append(self.theta-self.theta_[i])\n",
    "                \n",
    "                               \n",
    "           #now we calculate the weights\n",
    "           #we know that weight is the sum of dot product of g_i and g_j divided by a normalization factor. \n",
    "            \n",
    "            normalization_factor = 0\n",
    "            \n",
    "            for i in range(self.num_tasks):\n",
    "                for j in range(self.num_tasks):      \n",
    "                    normalization_factor += np.abs(np.dot(self.g[i].T, self.g[j]))\n",
    "                    \n",
    "            w = np.zeros(self.num_tasks)\n",
    "            \n",
    "            for i in range(self.num_tasks):\n",
    "\n",
    "                for j in range(self.num_tasks):\n",
    "                    w[i] += np.dot(self.g[i].T, self.g[j])\n",
    "\n",
    "                w[i] = w[i] / normalization_factor\n",
    "                \n",
    "                \n",
    "     \n",
    "            #initialize meta gradients\n",
    "            weighted_gradient = np.zeros(self.theta.shape)\n",
    "                        \n",
    "            for i in range(self.num_tasks):\n",
    "            \n",
    "                #sample k data points and prepare our test set for meta training\n",
    "                XTest, YTest = sample_points(10)\n",
    "\n",
    "                #predict the value of y\n",
    "                a = np.matmul(XTest, self.theta_[i])\n",
    "                \n",
    "                YPred = self.sigmoid(a)\n",
    "                           \n",
    "                #compute meta gradients\n",
    "                meta_gradient = np.matmul(XTest.T, (YPred - YTest)) / self.num_samples\n",
    "                \n",
    "                \n",
    "                weighted_gradient += np.sum(w[i]*meta_gradient)\n",
    "\n",
    "  \n",
    "            #update our randomly initialized model parameter theta with the meta gradients\n",
    "            self.theta = self.theta-self.beta*weighted_gradient/self.num_tasks\n",
    "                                       \n",
    "            if e%10==0:\n",
    "                print(\"Epoch {}: Loss {}\\n\".format(e,loss))\n",
    "                print('Updated Model Parameter Theta\\n')\n",
    "                print('Sampling Next Batch of Tasks \\n')\n",
    "                print('---------------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an instance of the Simple_MAML class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Simple_MAML()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss 0.8014744361048484\n",
      "\n",
      "Updated Model Parameter Theta\n",
      "\n",
      "Sampling Next Batch of Tasks \n",
      "\n",
      "---------------------------------\n",
      "\n",
      "Epoch 10: Loss 1.4315356246126996\n",
      "\n",
      "Updated Model Parameter Theta\n",
      "\n",
      "Sampling Next Batch of Tasks \n",
      "\n",
      "---------------------------------\n",
      "\n",
      "Epoch 20: Loss 1.345245725621392\n",
      "\n",
      "Updated Model Parameter Theta\n",
      "\n",
      "Sampling Next Batch of Tasks \n",
      "\n",
      "---------------------------------\n",
      "\n",
      "Epoch 30: Loss 1.50157957143627\n",
      "\n",
      "Updated Model Parameter Theta\n",
      "\n",
      "Sampling Next Batch of Tasks \n",
      "\n",
      "---------------------------------\n",
      "\n",
      "Epoch 40: Loss 2.075511601385821\n",
      "\n",
      "Updated Model Parameter Theta\n",
      "\n",
      "Sampling Next Batch of Tasks \n",
      "\n",
      "---------------------------------\n",
      "\n",
      "Epoch 50: Loss 0.8197531992020185\n",
      "\n",
      "Updated Model Parameter Theta\n",
      "\n",
      "Sampling Next Batch of Tasks \n",
      "\n",
      "---------------------------------\n",
      "\n",
      "Epoch 60: Loss 1.3705063935193043\n",
      "\n",
      "Updated Model Parameter Theta\n",
      "\n",
      "Sampling Next Batch of Tasks \n",
      "\n",
      "---------------------------------\n",
      "\n",
      "Epoch 70: Loss 1.8833644389294364\n",
      "\n",
      "Updated Model Parameter Theta\n",
      "\n",
      "Sampling Next Batch of Tasks \n",
      "\n",
      "---------------------------------\n",
      "\n",
      "Epoch 80: Loss 1.4405743747108266\n",
      "\n",
      "Updated Model Parameter Theta\n",
      "\n",
      "Sampling Next Batch of Tasks \n",
      "\n",
      "---------------------------------\n",
      "\n",
      "Epoch 90: Loss 1.7590141761587177\n",
      "\n",
      "Updated Model Parameter Theta\n",
      "\n",
      "Sampling Next Batch of Tasks \n",
      "\n",
      "---------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
